# -*- coding: utf-8 -*-
"""проект 1 семестр

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-BP1Y_ckWLMn30-Ug5n5hBoXt2ZWABV

1.Линейная регрессия (реализация с нуля).

1.1 Подготовка данных и визуализация
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import ipywidgets as widgets
from IPython.display import display

# Загрузка данных
df = pd.read_csv('instax_sales_transaction_data[1].csv')
print(f"Размер датасета: {df.shape}")
# Выберем признаки для регрессии
# X = Цена за единицу, y = Общая выручка
X = df[['Harga_Satuan']].values
y = df['Total_Penjualan'].values.reshape(-1, 1)

# Масштабирование данных (важно для сходимости градиентного спуска)
scaler_x = StandardScaler()
scaler_y = StandardScaler()
X_scaled = scaler_x.fit_transform(X)
y_scaled = scaler_y.fit_transform(y)

# Добавляем столбец единиц для интерсепта (bias)
X_b = np.c_[np.ones((len(X_scaled), 1)), X_scaled]

"""1.2 Реализация линейной регрессии (NumPy)"""

class CustomLinearRegression:
    def __init__(self, learning_rate=0.01, epochs=100):
        self.lr = learning_rate
        self.epochs = epochs
        self.weights = None
        self.history = []

    def fit(self, X, y):
        n_samples, n_features = X.shape
        # Инициализация весов нулями
        self.weights = np.zeros((n_features, 1))
        self.history = []

        for _ in range(self.epochs):
            # 1. Предсказание: y_hat = X * w
            y_hat = np.dot(X, self.weights)

            # 2. Ошибка: MSE
            error = y_hat - y
            loss = np.mean(error**2)
            self.history.append(loss)

            # 3. Градиент: (2/n) * X.T * error
            gradient = (2/n_samples) * np.dot(X.T, error)

            # 4. Обновление весов
            self.weights -= self.lr * gradient

    def predict(self, X):
        return np.dot(X, self.weights)

"""1.3 Интерактивная демонстрация (Виджеты)

"""

def plot_regression(lr, epochs):
    model = CustomLinearRegression(learning_rate=lr, epochs=epochs)
    model.fit(X_b, y_scaled)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # График 1: Scatter Plot и Линия регрессии
    sns.regplot(x=X_scaled.flatten(), y=y_scaled.flatten(), ax=ax1,
                scatter_kws={'alpha':0.3}, line_kws={'color':'red', 'label':'Regression Line'})
    ax1.set_title(f"Данные и Линия (LR: {lr})")
    ax1.set_xlabel("Цена (Scaled)")
    ax1.set_ylabel("Выручка (Scaled)")

    # График 2: Функция потерь (MSE)
    ax2.plot(range(len(model.history)), model.history, color='blue')
    ax2.set_title("График потерь (MSE) по эпохам")
    ax2.set_xlabel("Эпоха")
    ax2.set_ylabel("Loss")
    ax2.grid(True)

    plt.show()

    # Вывод коэффициентов
    intercept = model.weights[0][0]
    coef = model.weights[1][0]
    print(f"Результат обучения:")
    print(f"Интерсепт (w0): {intercept:.4f}")
    print(f"Коэффициент (w1): {coef:.4f}")
    print(f"Финальный MSE: {model.history[-1]:.6f}")

interact_plot = widgets.interactive(
    plot_regression,
    lr=widgets.FloatLogSlider(value=0.01, base=10, min=-4, max=0, step=0.1, description='Learn Rate'),
    epochs=widgets.IntSlider(value=50, min=10, max=1000, step=10, description='Epochs')
)

display(interact_plot)

"""2.Логистическая регрессия

2.1 Подготовка данных для классификации
"""

from sklearn.model_selection import train_test_split

# Создаем бинарную целевую переменную: 1 если Kartu Kredit, 0 если другой метод
df['Is_Credit_Card'] = (df['Metode_Bayar'] == 'Kartu Kredit').astype(int)

# Выберем признаки: Цена, Количество и Дисконт
X_cls = df[['Harga_Satuan', 'Qty', 'Diskon_IDR']].values
y_cls = df['Is_Credit_Card'].values.reshape(-1, 1)

# Масштабирование
scaler_cls = StandardScaler()
X_cls_scaled = scaler_cls.fit_transform(X_cls)
X_cls_b = np.c_[np.ones((X_cls_scaled.shape[0], 1)), X_cls_scaled]

X_train, X_test, y_train, y_test = train_test_split(X_cls_b, y_cls, test_size=0.2, random_state=42)

"""2.2 Реализация Логистической регрессии (NumPy)"""

class CustomLogisticRegression:
    def __init__(self, lr=0.01, epochs=1000, l2_reg=0.01):
        self.lr = lr
        self.epochs = epochs
        self.l2_reg = l2_reg
        self.weights = None
        self.losses = []

    def sigmoid(self, z):
        # Ограничиваем z, чтобы избежать переполнения exp
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    def compute_loss(self, y, y_pred):
        # Log-Loss (Кросс-энтропия) с небольшой добавкой 1e-15 во избежание log(0)
        n = len(y)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        loss = - (1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

        # L2 регуляризация (опционально по заданию)
        l2_penalty = (self.l2_reg / (2 * n)) * np.sum(self.weights[1:]**2)
        return loss + l2_penalty

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros((n_features, 1))
        self.losses = []

        for i in range(self.epochs):
            # Линейная комбинация
            z = np.dot(X, self.weights)
            # Активация сигмоидой
            y_pred = self.sigmoid(z)

            # Расчет потерь
            loss = self.compute_loss(y, y_pred)
            self.losses.append(loss)

            # Градиент с учетом L2 (регуляризация не применяется к интерсепту weights[0])
            error = y_pred - y
            gradient = (1/n_samples) * np.dot(X.T, error)

            # Добавляем градиент L2 (кроме смещения)
            l2_grad = (self.l2_reg / n_samples) * self.weights
            l2_grad[0] = 0

            # Обновление весов
            self.weights -= self.lr * (gradient + l2_grad)

    def predict_prob(self, X):
        return self.sigmoid(np.dot(X, self.weights))

    def predict(self, X, threshold=0.5):
        return (self.predict_prob(X) >= threshold).astype(int)

"""2.3 Интерактивная визуализация и метрики"""

def plot_logistic(lr, epochs, l2):
    model = CustomLogisticRegression(lr=lr, epochs=epochs, l2_reg=l2)
    model.fit(X_train, y_train)

    # Визуализация потерь
    plt.figure(figsize=(10, 5))
    plt.plot(model.losses, color='green')
    plt.title("Обучение логистической регрессии (Log-Loss)")
    plt.xlabel("Эпохи")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

    # Оценка
    preds = model.predict(X_test)
    accuracy = np.mean(preds == y_test)
    print(f"Точность (Accuracy) на тестовой выборке: {accuracy:.4f}")
    print(f"Финальные веса: \n{model.weights.flatten()}")

# Интерфейс
interact_cls = widgets.interactive(
    plot_logistic,
    lr=widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.1, description='LR'),
    epochs=widgets.IntSlider(value=500, min=100, max=2000, step=100, description='Epochs'),
    l2=widgets.FloatSlider(value=0.01, min=0, max=1, step=0.01, description='L2 Reg')
)

display(interact_cls)

"""3.Классификация (два подхода).

3.1 Подготовка данных и обучение моделей
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

# 1. Подготовка меток (Бинарная классификация)
# Допустим, 1 — безналичный расчет (Kartu Kredit, E-Wallet, Paylater), 0 — остальные
digital_methods = ['Kartu Kredit', 'E-Wallet (Gopay/OVO)', 'Paylater']
df['Is_Digital'] = df['Metode_Bayar'].apply(lambda x: 1 if x in digital_methods else 0)

# Признаки (X) и Цель (y)
X = df[['Harga_Satuan', 'Qty', 'Diskon_IDR']].values
y = df['Is_Digital'].values

# Разделение и масштабирование (как в прошлых шагах)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Добавляем bias для нашей логистической регрессии
X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]
X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]

# --- Обучение модели 1: Логистическая регрессия (с нуля) ---
log_reg = CustomLogisticRegression(lr=0.1, epochs=1000)
log_reg.fit(X_train_b, y_train.reshape(-1, 1))
y_pred_log = log_reg.predict(X_test_b)
y_prob_log = log_reg.predict_prob(X_test_b)

# --- Обучение модели 2: Random Forest (sklearn) ---
# Мы выбираем Random Forest, так как он лучше справляется с нелинейными зависимостями
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)
y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

"""3.2 Сравнение и метрики"""

def print_metrics(y_true, y_pred, y_prob, model_name):
    print(f"--- Метрики для {model_name} ---")
    print(f"Accuracy:  {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall:    {recall_score(y_true, y_pred):.4f}")
    print(f"F1-Score:  {f1_score(y_true, y_pred):.4f}")
    print(f"ROC AUC:   {roc_auc_score(y_true, y_prob):.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("-" * 30)

print_metrics(y_test, y_pred_log, y_prob_log, "Логистическая регрессия (Handmade)")
print_metrics(y_test, y_pred_rf, y_prob_rf, "Random Forest (Sklearn)")

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 1. Инициализация и обучение решающего дерева
# Ограничим глубину (max_depth), чтобы дерево было читаемым на защите
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train_scaled, y_train)

# 2. Предсказания
y_pred_dt = dt_model.predict(X_test_scaled)
y_prob_dt = dt_model.predict_proba(X_test_scaled)[:, 1]

# 3. Сравнение Accuracy
acc_log = accuracy_score(y_test, y_pred_log)
acc_dt = accuracy_score(y_test, y_pred_dt)

print(f"Accuracy Логистической регрессии (с нуля): {acc_log:.4f}")
print(f"Accuracy Решающего дерева (sklearn): {acc_dt:.4f}")

plt.figure(figsize=(20, 10))
plot_tree(dt_model,
          feature_names=['Harga_Satuan', 'Qty', 'Diskon_IDR'],
          class_names=['Other', 'Digital'],
          filled=True,
          rounded=True,
          fontsize=12)
plt.title("Визуализация структуры Решающего дерева")
plt.show()

from sklearn.metrics import roc_curve, auc

def plot_roc_comparison(y_test, y_prob_log, y_prob_dt):
    fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)
    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr_log, tpr_log, label=f'LogReg (AUC = {auc(fpr_log, tpr_log):.2f})')
    plt.plot(fpr_dt, tpr_dt, label=f'DecTree (AUC = {auc(fpr_dt, tpr_dt):.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Сравнение ROC AUC')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_roc_comparison(y_test, y_prob_log, y_prob_dt)

"""4.Эксперименты и метрики.

4.1 Расширенный анализ метрик
"""

from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix, roc_curve

def evaluate_model(y_true, y_pred, y_prob, name):
    # Расчет метрик
    acc = accuracy_score(y_true, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
    auc_val = roc_auc_score(y_true, y_prob)

    print(f"\n--- Метрики для {name} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"ROC AUC:   {auc_val:.4f}")

    # Визуализация Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {name}')
    plt.ylabel('Реальные значения')
    plt.xlabel('Предсказанные значения')
    plt.show()

# Оценка наших моделей
evaluate_model(y_test, y_pred_log, y_prob_log, "Логистическая регрессия (Custom)")
evaluate_model(y_test, y_pred_dt, y_prob_dt, "Decision Tree (Sklearn)")

"""4.2 Эксперимент: Влияние Learning Rate и Эпох на сходимость"""

def run_experiments():
    rates = [0.1, 0.01, 0.001]
    plt.figure(figsize=(12, 6))

    for lr in rates:
        model = CustomLinearRegression(learning_rate=lr, epochs=200)
        model.fit(X_b, y_scaled)
        plt.plot(model.history, label=f'LR = {lr}')

    plt.title('Влияние Learning Rate на сходимость (Linear Regression)')
    plt.xlabel('Эпохи')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

run_experiments()