{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instax Sales Transaction Analysis\n",
    "\n",
    "This notebook serves as the main interactive environment for the Instax Sales Transaction Machine Learning project. It includes all parts of the project, from data loading and preprocessing to model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.preprocessing import load_and_preprocess_data\n",
    "from src.linear_regression import LinearRegression\n",
    "from src.logistic_regression import LogisticRegression\n",
    "from src.experiments import run_experiments\n",
    "from src.widgets_interface import create_widgets\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = load_and_preprocess_data('data/instax_sales_transaction_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we will perform EDA to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "\n",
    "# Missing values overview\n",
    "missing_values = data.isnull().sum()\n",
    "print('Missing values overview:')\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Numeric and categorical feature summary\n",
    "numeric_summary = data.describe()\n",
    "categorical_summary = data.select_dtypes(include=['object']).describe()\n",
    "print('Numeric feature summary:')\n",
    "print(numeric_summary)\n",
    "print('Categorical feature summary:')\n",
    "print(categorical_summary)\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "In this section, we will clean the dataset by handling missing values, converting dates, and creating new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data.fillna(method='ffill', inplace=True)  # Forward fill for simplicity\n",
    "\n",
    "# Convert dates to datetime\n",
    "data['transaction_date'] = pd.to_datetime(data['transaction_date'])\n",
    "\n",
    "# Create new features\n",
    "data['month'] = data['transaction_date'].dt.month\n",
    "data['day'] = data['transaction_date'].dt.day\n",
    "data['season'] = data['transaction_date'].dt.month % 12 // 3 + 1\n",
    "data['revenue'] = data['quantity'] * data['price']\n",
    "data['profit'] = data['revenue'] - data['cost']\n",
    "\n",
    "# Encode categorical features\n",
    "data = pd.get_dummies(data, columns=['category'], drop_first=True)\n",
    "\n",
    "# Split dataset for regression and classification tasks\n",
    "X = data.drop(['total_revenue'], axis=1)\n",
    "y_regression = data['total_revenue']\n",
    "y_classification = (data['total_revenue'] > data['total_revenue'].median()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In this section, we will implement linear regression from scratch using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the linear regression model\n",
    "linear_model = LinearRegression(learning_rate=0.01, epochs=1000)\n",
    "linear_model.fit(X, y_regression)\n",
    "\n",
    "# Plot MSE vs epochs\n",
    "plt.plot(linear_model.losses)\n",
    "plt.title('MSE vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section, we will implement logistic regression from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression model\n",
    "logistic_model = LogisticRegression(learning_rate=0.01, epochs=1000)\n",
    "logistic_model.fit(X, y_classification)\n",
    "\n",
    "# Plot training loss vs epochs\n",
    "plt.plot(logistic_model.losses)\n",
    "plt.title('Training Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "In this section, we will evaluate the models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predictions\n",
    "y_pred_class = logistic_model.predict(X)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_classification, y_pred_class)\n",
    "precision = precision_score(y_classification, y_pred_class)\n",
    "recall = recall_score(y_classification, y_pred_class)\n",
    "f1 = f1_score(y_classification, y_pred_class)\n",
    "conf_matrix = confusion_matrix(y_classification, y_pred_class)\n",
    "roc_auc = roc_auc_score(y_classification, y_pred_class)\n",
    "\n",
    "# Display metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print(f'ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Interface\n",
    "\n",
    "In this section, we will create an interactive interface using ipywidgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets\n",
    "create_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have performed data loading, preprocessing, exploratory data analysis, and implemented linear and logistic regression models from scratch. We also evaluated the models and created an interactive interface for further exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}